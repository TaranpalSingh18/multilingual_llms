{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976bc734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\OneDrive\\Desktop\\intent_classification\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Punjabi CSV saved as: is_test_punjabi.csv\n"
     ]
    }
   ],
   "source": [
    "#Code for translating in punjabi first, then backtranslating into english and then comparing the cosine similarities of both the english sentences *the ground truth vs the backtranslated one) and then adding up those sentences which have threshold above 0.85\n",
    "#done for training data\n",
    "\n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# ─── 1) CONFIGURATION ────────────────────────────────────────────────────────\n",
    "INPUT_CSV_PATH      = \"is_test.csv\"           # Your English CSV\n",
    "OUTPUT_CSV_PATH     = \"is_test_punjabi.csv\"   # Output Punjabi CSV\n",
    "SIMILARITY_THRESHOLD = 0.85\n",
    "\n",
    "# Initialize Deep-Translator’s GoogleTranslator and SBERT\n",
    "#   - source='en', target='pa'  means English → Punjabi\n",
    "#   - source='pa', target='en'  means Punjabi → English\n",
    "eng_to_pa = GoogleTranslator(source='en', target='pa')\n",
    "pa_to_eng = GoogleTranslator(source='pa', target='en')\n",
    "\n",
    "# SBERT model for computing cosine similarity\n",
    "sbert = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# ─── 2) READ INPUT CSV ───────────────────────────────────────────────────────\n",
    "df = pd.read_csv(INPUT_CSV_PATH, dtype=str)  # Read all columns as string\n",
    "text_columns = list(df.columns)\n",
    "\n",
    "# ─── 3) TRANSLATE, BACK-TRANSLATE & FILTER ─────────────────────────────────\n",
    "for col in text_columns:\n",
    "    punjabi_texts = []\n",
    "    back_translated_texts = []\n",
    "\n",
    "    # a) Translate each sentence to Punjabi, then back to English\n",
    "    for sentence in df[col].fillna(\"\").tolist():\n",
    "        # English → Punjabi\n",
    "        try:\n",
    "            pu_translation = eng_to_pa.translate(sentence)\n",
    "        except Exception as e:\n",
    "            pu_translation = \"\"\n",
    "        punjabi_texts.append(pu_translation)\n",
    "\n",
    "        # Punjabi → English (back-translation)\n",
    "        try:\n",
    "            back_eng = pa_to_eng.translate(pu_translation)\n",
    "        except Exception as e:\n",
    "            back_eng = \"\"\n",
    "        back_translated_texts.append(back_eng)\n",
    "\n",
    "    # b) Compute SBERT embeddings & cosine similarities\n",
    "    originals = df[col].fillna(\"\").tolist()\n",
    "    # If all original or back-translated texts are empty, skip similarity computation\n",
    "    if any(originals) and any(back_translated_texts):\n",
    "        embeddings_orig = sbert.encode(originals, convert_to_tensor=True)\n",
    "        embeddings_back = sbert.encode(back_translated_texts, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(embeddings_orig, embeddings_back).diag().cpu().numpy()\n",
    "    else:\n",
    "        cosine_scores = [0.0] * len(originals)\n",
    "\n",
    "    # c) Filter: keep Punjabi translation if similarity ≥ threshold, else blank\n",
    "    filtered_punjabi = [\n",
    "        pu if score >= SIMILARITY_THRESHOLD else \"\"\n",
    "        for pu, score in zip(punjabi_texts, cosine_scores)\n",
    "    ]\n",
    "\n",
    "    # d) Add a new column named \"<original_column>_pa\"\n",
    "    df[f\"{col}_pa\"] = filtered_punjabi\n",
    "\n",
    "# ─── 4) SAVE OUTPUT CSV ──────────────────────────────────────────────────────\n",
    "df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "print(f\"✅ Punjabi CSV saved as: {OUTPUT_CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70350185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 20:32:07 - INFO - Script started in mode=both\n",
      "2025-06-12 20:32:07 - INFO - Loading data from is_test.csv\n",
      "2025-06-12 20:32:07 - INFO - Initializing translators and SBERT model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2025-06-12 20:32:07 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-12 20:32:07 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2025-06-12 20:32:12 - INFO - Processing column: '1' (4500 sentences)\n",
      "2025-06-12 20:32:12 - INFO - Translating to Punjabi and back-translating to English with retries...\n",
      "Translating '1':  13%|█▎        | 595/4500 [19:21<2:24:43,  2.22s/sent]2025-06-12 20:51:55 - WARNING - [en->pa] translation attempt #1 failed: HTTPSConnectionPool(host='translate.google.com', port=443): Max retries exceeded with url: /m?tl=pa&sl=en&q=what+region+were+you+born (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E144D2BD90>, 'Connection to translate.google.com timed out. (connect timeout=None)'))\n",
      "Translating '1':  13%|█▎        | 601/4500 [19:53<3:05:13,  2.85s/sent]2025-06-12 21:45:45 - WARNING - [pa->en] translation attempt #1 failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Translating '1':  20%|█▉        | 895/4500 [1:25:52<2:25:29,  2.42s/sent]    2025-06-12 21:58:09 - WARNING - [pa->en] translation attempt #1 failed: ਇਹ ਡਿਜ਼ਾਇਨ ਕਿਹੜੀ ਕੰਪਨੀ ਨੇ ਕੀਤਾ --> No translation was found using the current translator. Try another translator?\n",
      "Translating '1':  26%|██▌       | 1155/4500 [1:37:33<6:36:53,  7.12s/sent]2025-06-12 22:10:07 - WARNING - [en->pa] translation attempt #1 failed: HTTPSConnectionPool(host='translate.google.com', port=443): Max retries exceeded with url: /m?tl=pa&sl=en&q=have+i+gone+over+my+entertainment+budget (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E144D2BD90>, 'Connection to translate.google.com timed out. (connect timeout=None)'))\n",
      "Translating '1':  27%|██▋       | 1201/4500 [1:39:37<1:24:41,  1.54s/sent] 2025-06-12 22:12:11 - WARNING - [en->pa] translation attempt #1 failed: HTTPSConnectionPool(host='translate.google.com', port=443): Max retries exceeded with url: /m?tl=pa&sl=en&q=tell+me+what+my+credit+score+is (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E14106DA90>, 'Connection to translate.google.com timed out. (connect timeout=None)'))\n",
      "Translating '1':  28%|██▊       | 1277/4500 [1:43:21<2:53:56,  3.24s/sent]2025-06-12 22:15:36 - WARNING - [pa->en] translation attempt #1 failed: ਮੈਨੂੰ ਲਗਦਾ ਹੈ ਕਿ ਮੇਰਾ ਸਮਾਨ ਗੁੰਮ ਗਿਆ ਹੈ --> No translation was found using the current translator. Try another translator?\n",
      "Translating '1':  37%|███▋      | 1657/4500 [2:00:54<2:05:47,  2.65s/sent]2025-06-12 22:33:29 - WARNING - [pa->en] translation attempt #1 failed: HTTPSConnectionPool(host='translate.google.com', port=443): Max retries exceeded with url: /m?tl=en&sl=pa&q=%E0%A8%AE%E0%A9%88%E0%A8%95%E0%A8%A1%E0%A9%8B%E0%A8%A8%E0%A8%B2%E0%A8%A1+%E0%A8%95%E0%A9%80+%E0%A8%B8%E0%A8%BF%E0%A8%B9%E0%A8%A4%E0%A8%AE%E0%A9%B0%E0%A8%A6+%E0%A8%B9%E0%A9%88 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001E14106DA90>, 'Connection to translate.google.com timed out. (connect timeout=None)'))\n",
      "Translating '1':  38%|███▊      | 1718/4500 [2:04:05<1:59:36,  2.58s/sent]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "from sentence_transformers import SentenceTransformer, util, losses, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_thresholds(scores: np.ndarray):\n",
    "    valid = scores[scores > 0]\n",
    "    if len(valid) == 0:\n",
    "        return {}\n",
    "    T15 = np.percentile(valid, 15)\n",
    "    T25 = np.percentile(valid, 25)\n",
    "    T50 = np.percentile(valid, 50)\n",
    "    T75 = np.percentile(valid, 75)\n",
    "    Tmin = T25 - 1.5 * (T75 - T25)\n",
    "    return {'T15': T15, 'T25': T25, 'T50': T50, 'Tmin': Tmin}\n",
    "\n",
    "\n",
    "def select_threshold(th: dict, choice: str):\n",
    "    if choice in th:\n",
    "        return th[choice]\n",
    "    raise ValueError(f\"Unknown threshold choice {choice}\")\n",
    "\n",
    "\n",
    "def translate_with_retry(translator, text, desc, max_retries=None, delay=1):\n",
    "    \"\"\"\n",
    "    Retry translation until success (non-empty) or until max_retries if specified.\n",
    "    \"\"\"\n",
    "    attempts = 0\n",
    "    while True:\n",
    "        try:\n",
    "            result = translator.translate(text)\n",
    "            if result:\n",
    "                return result\n",
    "            else:\n",
    "                raise ValueError(\"Empty translation\")\n",
    "        except Exception as e:\n",
    "            attempts += 1\n",
    "            logging.warning(f\"[{desc}] translation attempt #{attempts} failed: {e}\")\n",
    "            if max_retries and attempts >= max_retries:\n",
    "                logging.error(f\"[{desc}] reached max retries={max_retries}. Returning empty string.\")\n",
    "                return \"\"\n",
    "            time.sleep(delay)\n",
    "\n",
    "\n",
    "def translate_and_filter(\n",
    "    input_csv: str,\n",
    "    output_csv: str,\n",
    "    threshold_choice: str = 'T25',\n",
    "    sample_below: int = 50,\n",
    "    model_name: str = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "):\n",
    "    setup_logging()\n",
    "    logging.info(f\"Loading data from {input_csv}\")\n",
    "    df = pd.read_csv(input_csv, dtype=str)\n",
    "    columns = df.columns.tolist()\n",
    "\n",
    "    logging.info(f\"Initializing translators and SBERT model: {model_name}\")\n",
    "    eng_to_pa = GoogleTranslator(source='en', target='pa')\n",
    "    pa_to_eng = GoogleTranslator(source='pa', target='en')\n",
    "    sbert = SentenceTransformer(model_name)\n",
    "\n",
    "    for col in columns:\n",
    "        logging.info(f\"Processing column: '{col}' ({len(df)} sentences)\")\n",
    "        originals = df[col].fillna(\"\").tolist()\n",
    "        punjabi_texts = []\n",
    "        back_translated = []\n",
    "\n",
    "        logging.info(\"Translating to Punjabi and back-translating to English with retries...\")\n",
    "        for sent in tqdm(originals, desc=f\"Translating '{col}'\", unit=\"sent\"):\n",
    "            pu = translate_with_retry(eng_to_pa, sent, desc=\"en->pa\")\n",
    "            punjabi_texts.append(pu)\n",
    "\n",
    "            back = translate_with_retry(pa_to_eng, pu, desc=\"pa->en\")\n",
    "            back_translated.append(back)\n",
    "\n",
    "        logging.info(\"Computing SBERT embeddings and cosine similarities...\")\n",
    "        if any(originals) and any(back_translated):\n",
    "            emb_o = sbert.encode(originals, convert_to_tensor=True)\n",
    "            emb_b = sbert.encode(back_translated, convert_to_tensor=True)\n",
    "            scores = util.cos_sim(emb_o, emb_b).diag().cpu().numpy()\n",
    "        else:\n",
    "            scores = np.zeros(len(originals))\n",
    "\n",
    "        ths = compute_thresholds(scores)\n",
    "        thr = select_threshold(ths, threshold_choice)\n",
    "        logging.info(f\"Thresholds: {ths}; Selected ({threshold_choice}) = {thr:.3f}\")\n",
    "\n",
    "        below_idxs = [i for i, sc in enumerate(scores) if sc < thr]\n",
    "        sample_n = min(len(below_idxs), sample_below)\n",
    "        sampled_below = random.sample(below_idxs, sample_n) if sample_n > 0 else []\n",
    "        logging.info(f\"Sampling {len(sampled_below)} below-threshold sentences (out of {len(below_idxs)})\")\n",
    "\n",
    "        filtered = []\n",
    "        for idx, (pu, sc) in enumerate(zip(punjabi_texts, scores)):\n",
    "            if sc >= thr or idx in sampled_below:\n",
    "                filtered.append(pu)\n",
    "            else:\n",
    "                filtered.append(\"\")\n",
    "        df[f\"{col}_pa\"] = filtered\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    logging.info(f\"Filtered translations saved to {output_csv}\")\n",
    "\n",
    "\n",
    "def fine_tune_sbert(\n",
    "    parallel_csv: str,\n",
    "    en_col: str,\n",
    "    pa_col: str,\n",
    "    pretrained_model: str = 'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    output_path: str = 'fine-tuned-en-pa-sbert',\n",
    "    batch_size: int = 16,\n",
    "    epochs: int = 3\n",
    "):\n",
    "    logging.info(f\"Loading parallel corpus from {parallel_csv}\")\n",
    "    df = pd.read_csv(parallel_csv, dtype=str)\n",
    "    examples = []\n",
    "    for idx, row in df.iterrows():\n",
    "        en = row.get(en_col, '').strip()\n",
    "        pa = row.get(pa_col, '').strip()\n",
    "        if en and pa:\n",
    "            examples.append(InputExample(texts=[en, pa]))\n",
    "        else:\n",
    "            logging.debug(f\"Skipping invalid row {idx}: en='{en}', pa='{pa}'\")\n",
    "\n",
    "    if not examples:\n",
    "        logging.error(\"No valid parallel examples found; aborting fine-tuning.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Initializing SBERT model for fine-tuning: {pretrained_model}\")\n",
    "    model = SentenceTransformer(pretrained_model)\n",
    "    loader = DataLoader(examples, shuffle=True, batch_size=batch_size)\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "    logging.info(f\"Starting fine-tuning: epochs={epochs}, batch_size={batch_size}\")\n",
    "    model.fit(\n",
    "        train_objectives=[(loader, train_loss)],\n",
    "        epochs=epochs,\n",
    "        warmup_steps=int(len(examples) * epochs * 0.1),\n",
    "        output_path=output_path,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    logging.info(f\"Model fine-tuned and saved to {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Translate/filter (with retries) and fine-tune SBERT on English–Punjabi data')\n",
    "    parser.add_argument('--mode', choices=['translate', 'finetune', 'both'], default='both')\n",
    "    parser.add_argument('--input_csv', type=str, default='is_test.csv')\n",
    "    parser.add_argument('--output_csv', type=str, default='is_test_punjabi.csv')\n",
    "    parser.add_argument('--parallel_csv', type=str, default='en_pa_parallel.csv')\n",
    "    parser.add_argument('--en_col', type=str, default='english')\n",
    "    parser.add_argument('--pa_col', type=str, default='punjabi')\n",
    "    parser.add_argument('--threshold', type=str, default='T25', help='one of T15,T25,T50,Tmin')\n",
    "    parser.add_argument('--sample_below', type=int, default=50, help='how many low-sim samples to keep')\n",
    "    parser.add_argument('--epochs', type=int, default=3)\n",
    "    parser.add_argument('--batch_size', type=int, default=16)\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    logging.info(f\"Script started in mode={args.mode}\")\n",
    "\n",
    "    if args.mode in ('translate', 'both'):\n",
    "        translate_and_filter(\n",
    "            args.input_csv,\n",
    "            args.output_csv,\n",
    "            threshold_choice=args.threshold,\n",
    "            sample_below=args.sample_below\n",
    "        )\n",
    "\n",
    "    if args.mode in ('finetune', 'both'):\n",
    "        fine_tune_sbert(\n",
    "            args.parallel_csv,\n",
    "            args.en_col,\n",
    "            args.pa_col,\n",
    "            pretrained_model='paraphrase-multilingual-MiniLM-L12-v2',\n",
    "            output_path='fine-tuned-en-pa-sbert',\n",
    "            batch_size=args.batch_size,\n",
    "            epochs=args.epochs\n",
    "        )\n",
    "    logging.info(\"Script finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9374e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\OneDrive\\Desktop\\intent_classification\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Punjabi CSV saved as: 200_sampled_output.csv\n"
     ]
    }
   ],
   "source": [
    "#same done on testing data set, initially contained 1000 values, cut down to 500 values\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# ─── 1) CONFIGURATION ────────────────────────────────────────────────────────\n",
    "INPUT_CSV_PATH      = \"200_sampled.csv\"           # Your English CSV\n",
    "OUTPUT_CSV_PATH     = \"200_sampled_output.csv\"   # Output Punjabi CSV\n",
    "SIMILARITY_THRESHOLD = 0.85\n",
    "\n",
    "# Initialize Deep-Translator’s GoogleTranslator and SBERT\n",
    "#   - source='en', target='pa'  means English → Punjabi\n",
    "#   - source='pa', target='en'  means Punjabi → English\n",
    "eng_to_pa = GoogleTranslator(source='en', target='pa')\n",
    "pa_to_eng = GoogleTranslator(source='pa', target='en')\n",
    "\n",
    "# SBERT model for computing cosine similarity\n",
    "sbert = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# ─── 2) READ INPUT CSV ───────────────────────────────────────────────────────\n",
    "df = pd.read_csv(INPUT_CSV_PATH, dtype=str)  # Read all columns as string\n",
    "text_columns = list(df.columns)\n",
    "\n",
    "# ─── 3) TRANSLATE, BACK-TRANSLATE & FILTER ─────────────────────────────────\n",
    "for col in text_columns:\n",
    "    punjabi_texts = []\n",
    "    back_translated_texts = []\n",
    "\n",
    "    # a) Translate each sentence to Punjabi, then back to English\n",
    "    for sentence in df[col].fillna(\"\").tolist():\n",
    "        # English → Punjabi\n",
    "        try:\n",
    "            pu_translation = eng_to_pa.translate(sentence)\n",
    "        except Exception as e:\n",
    "            pu_translation = \"\"\n",
    "        punjabi_texts.append(pu_translation)\n",
    "\n",
    "        # Punjabi → English (back-translation)\n",
    "        try:\n",
    "            back_eng = pa_to_eng.translate(pu_translation)\n",
    "        except Exception as e:\n",
    "            back_eng = \"\"\n",
    "        back_translated_texts.append(back_eng)\n",
    "\n",
    "    # b) Compute SBERT embeddings & cosine similarities\n",
    "    originals = df[col].fillna(\"\").tolist()\n",
    "    # If all original or back-translated texts are empty, skip similarity computation\n",
    "    if any(originals) and any(back_translated_texts):\n",
    "        embeddings_orig = sbert.encode(originals, convert_to_tensor=True)\n",
    "        embeddings_back = sbert.encode(back_translated_texts, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(embeddings_orig, embeddings_back).diag().cpu().numpy()\n",
    "    else:\n",
    "        cosine_scores = [0.0] * len(originals)\n",
    "\n",
    "    # c) Filter: keep Punjabi translation if similarity ≥ threshold, else blank\n",
    "    filtered_punjabi = [\n",
    "        pu if score >= SIMILARITY_THRESHOLD else \"\"\n",
    "        for pu, score in zip(punjabi_texts, cosine_scores)\n",
    "    ]\n",
    "\n",
    "    # d) Add a new column named \"<original_column>_pa\"\n",
    "    df[f\"{col}_pa\"] = filtered_punjabi\n",
    "\n",
    "# ─── 4) SAVE OUTPUT CSV ──────────────────────────────────────────────────────\n",
    "df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "print(f\"✅ Punjabi CSV saved as: {OUTPUT_CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a824ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#removing those rows having unfilled cells\n",
    "import numpy as np\n",
    "pa_cols = [c for c in df.columns if c.endswith(\"_pa\")]\n",
    "\n",
    "# replace empty with NaN\n",
    "df.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "# drop rows where **any** of the pa columns is NaN\n",
    "df.dropna(subset=pa_cols, how=\"any\", inplace=True)\n",
    "\n",
    "# save\n",
    "df.to_excel(\"sampled.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30d43a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#randomly picking up 200 from the csv\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "input_path = \"sampled.xlsx\"\n",
    "output_path = \"final_sampled.xlsx\"\n",
    "\n",
    "samples = 200\n",
    "\n",
    "df = pd.read_excel(input_path)\n",
    "sampled_df= df.sample(samples, random_state=42)\n",
    "\n",
    "sampled_df.to_excel(output_path, index= False)\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
