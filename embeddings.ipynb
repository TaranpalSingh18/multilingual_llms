{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a2ac84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding intents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  4.16it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  4.32it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  4.23it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing distances...\n",
      "âœ… Comparison complete! Saved to Comparision_Intents.xlsx and Comparision_Intents.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "\n",
    "#Code for embedding and comparision of intents from ground truth and gemini, sarvam and gpt data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Load Sentence-BERT multilingual model\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Load the datasets\n",
    "ground_truth_df = pd.read_excel(\"unseen_data1xlsx.xlsx\")\n",
    "gpt_df = pd.read_excel(\"Gpt_Unseen.xlsx\")\n",
    "sarvam_df = pd.read_excel(\"Sarvam_Unseen.xlsx\")\n",
    "gemini_df = pd.read_excel(\"Gemini_Unseen.xlsx\")\n",
    "\n",
    "# Extract intent columns\n",
    "ground_truth_intents = ground_truth_df['002_pa'].astype(str).tolist()\n",
    "gpt_intents = gpt_df['Intent'].astype(str).tolist()\n",
    "sarvam_intents = sarvam_df['Intent'].astype(str).tolist()\n",
    "gemini_intents = gemini_df['Intent'].astype(str).tolist()\n",
    "\n",
    "# Encode all intents using SBERT\n",
    "print(\"Encoding intents...\")\n",
    "\n",
    "ground_truth_embeddings = model.encode(ground_truth_intents, show_progress_bar=True)\n",
    "gpt_embeddings = model.encode(gpt_intents, show_progress_bar=True)\n",
    "sarvam_embeddings = model.encode(sarvam_intents, show_progress_bar=True)\n",
    "gemini_embeddings = model.encode(gemini_intents, show_progress_bar=True)\n",
    "\n",
    "# Compute Euclidean distances\n",
    "print(\"Computing distances...\")\n",
    "\n",
    "gpt_distances = []\n",
    "sarvam_distances = []\n",
    "gemini_distances = []\n",
    "\n",
    "for i in range(len(ground_truth_embeddings)):\n",
    "    gpt_distances.append(euclidean(ground_truth_embeddings[i], gpt_embeddings[i]))\n",
    "    sarvam_distances.append(euclidean(ground_truth_embeddings[i], sarvam_embeddings[i]))\n",
    "    gemini_distances.append(euclidean(ground_truth_embeddings[i], gemini_embeddings[i]))\n",
    "\n",
    "# Aggregate total and average distances\n",
    "gpt_total_distance = np.sum(gpt_distances)\n",
    "sarvam_total_distance = np.sum(sarvam_distances)\n",
    "gemini_total_distance = np.sum(gemini_distances)\n",
    "\n",
    "gpt_avg_distance = np.mean(gpt_distances)\n",
    "sarvam_avg_distance = np.mean(sarvam_distances)\n",
    "gemini_avg_distance = np.mean(gemini_distances)\n",
    "\n",
    "# Prepare summary table\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model': ['GPT', 'Sarvam', 'Gemini'],\n",
    "    'Total Distance': [gpt_total_distance, sarvam_total_distance, gemini_total_distance],\n",
    "    'Average Distance': [gpt_avg_distance, sarvam_avg_distance, gemini_avg_distance]\n",
    "})\n",
    "\n",
    "# Add Ranking (1 = best)\n",
    "summary_df['Rank'] = summary_df['Average Distance'].rank(method='min')\n",
    "\n",
    "# Save to Excel / CSV\n",
    "summary_df.to_excel(\"Comparision_Intents.xlsx\", index=False)\n",
    "summary_df.to_csv(\"Comparision_Intents.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Comparison complete! Saved to Comparision_Intents.xlsx and Comparision_Intents.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe50427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding intents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.46it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  3.56it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  4.26it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:03<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarities...\n",
      "âœ… Cosine-based comparison complete! Saved to Comparison_Intents_Cosine.*\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # â† new import\n",
    "\n",
    "# Load Sentence-BERT multilingual model\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Load the datasets\n",
    "ground_truth_df = pd.read_excel(\"unseen_data1xlsx.xlsx\")\n",
    "gpt_df = pd.read_excel(\"Gpt_Unseen.xlsx\")\n",
    "sarvam_df = pd.read_excel(\"Sarvam_Unseen.xlsx\")\n",
    "gemini_df = pd.read_excel(\"Gemini_Unseen.xlsx\")\n",
    "\n",
    "# Extract intent columns\n",
    "ground_truth_intents = ground_truth_df['002_pa'].astype(str).tolist()\n",
    "gpt_intents          = gpt_df['Intent'].astype(str).tolist()\n",
    "sarvam_intents       = sarvam_df['Intent'].astype(str).tolist()\n",
    "gemini_intents       = gemini_df['Intent'].astype(str).tolist()\n",
    "\n",
    "# Encode all intents using SBERT\n",
    "print(\"Encoding intents...\")\n",
    "ground_truth_embeddings = model.encode(ground_truth_intents, show_progress_bar=True)\n",
    "gpt_embeddings          = model.encode(gpt_intents,          show_progress_bar=True)\n",
    "sarvam_embeddings       = model.encode(sarvam_intents,       show_progress_bar=True)\n",
    "gemini_embeddings       = model.encode(gemini_intents,       show_progress_bar=True)\n",
    "\n",
    "# Compute Cosine similarities\n",
    "print(\"Computing cosine similarities...\")\n",
    "\n",
    "gpt_sims    = []\n",
    "sarvam_sims = []\n",
    "gemini_sims = []\n",
    "\n",
    "for i in range(len(ground_truth_embeddings)):\n",
    "    # reshape to 2D arrays for sklearn\n",
    "    gt_vec    = ground_truth_embeddings[i].reshape(1, -1)\n",
    "    gpt_vec   = gpt_embeddings[i].reshape(1, -1)\n",
    "    sarvam_vec= sarvam_embeddings[i].reshape(1, -1)\n",
    "    gemini_vec= gemini_embeddings[i].reshape(1, -1)\n",
    "\n",
    "    # cosine_similarity returns a 1Ã—1 matrix\n",
    "    gpt_sims.append    (cosine_similarity(gt_vec, gpt_vec)[0,0])\n",
    "    sarvam_sims.append (cosine_similarity(gt_vec, sarvam_vec)[0,0])\n",
    "    gemini_sims.append (cosine_similarity(gt_vec, gemini_vec)[0,0])\n",
    "\n",
    "# If you prefer to turn similarity into a distance:  \n",
    "# gpt_sims    = [1 - s for s in gpt_sims]\n",
    "# sarvam_sims = [1 - s for s in sarvam_sims]\n",
    "# gemini_sims = [1 - s for s in gemini_sims]\n",
    "\n",
    "# Aggregate total and average similarities\n",
    "gpt_total_sim    = np.sum(gpt_sims)\n",
    "sarvam_total_sim = np.sum(sarvam_sims)\n",
    "gemini_total_sim = np.sum(gemini_sims)\n",
    "\n",
    "gpt_avg_sim    = np.mean(gpt_sims)\n",
    "sarvam_avg_sim = np.mean(sarvam_sims)\n",
    "gemini_avg_sim = np.mean(gemini_sims)\n",
    "\n",
    "# Prepare summary table\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model':             ['GPT', 'Sarvam', 'Gemini'],\n",
    "    'Total Similarity':  [gpt_total_sim, sarvam_total_sim, gemini_total_sim],\n",
    "    'Average Similarity':[gpt_avg_sim, sarvam_avg_sim, gemini_avg_sim]\n",
    "})\n",
    "\n",
    "# Add Ranking (1 = best, i.e. highest average similarity)\n",
    "summary_df['Rank'] = (-summary_df['Average Similarity']).rank(method='min').astype(int)\n",
    "\n",
    "# Save to Excel / CSV\n",
    "summary_df.to_excel(\"Comparison_Intents_Cosine.xlsx\", index=False)\n",
    "summary_df.to_csv(\"Comparison_Intents_Cosine.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Cosine-based comparison complete! Saved to Comparison_Intents_Cosine.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75305fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding intents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  1.00s/it]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.73it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:03<00:00,  2.06it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarities...\n",
      "âœ… Done! Comparison saved to Intent_Similarity_Comparison_LaBSE.xlsx and .csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## code using LaBse model for embedding and comparisions \n",
    "\n",
    "# Required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load LaBSE model\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "\n",
    "# Load the datasets\n",
    "ground_truth_df = pd.read_excel(\"unseen_data1xlsx.xlsx\")\n",
    "gpt_df = pd.read_excel(\"Gpt_Unseen.xlsx\")\n",
    "sarvam_df = pd.read_excel(\"Sarvam_Unseen.xlsx\")\n",
    "gemini_df = pd.read_excel(\"Gemini_Unseen.xlsx\")\n",
    "\n",
    "# Extract intent columns as string\n",
    "ground_truth_intents = ground_truth_df['002_pa'].astype(str).tolist()\n",
    "gpt_intents = gpt_df['Intent'].astype(str).tolist()\n",
    "sarvam_intents = sarvam_df['Intent'].astype(str).tolist()\n",
    "gemini_intents = gemini_df['Intent'].astype(str).tolist()\n",
    "\n",
    "# Encode all intents using LaBSE\n",
    "print(\"Encoding intents...\")\n",
    "\n",
    "ground_truth_embeddings = model.encode(ground_truth_intents, convert_to_tensor=True, show_progress_bar=True)\n",
    "gpt_embeddings = model.encode(gpt_intents, convert_to_tensor=True, show_progress_bar=True)\n",
    "sarvam_embeddings = model.encode(sarvam_intents, convert_to_tensor=True, show_progress_bar=True)\n",
    "gemini_embeddings = model.encode(gemini_intents, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Compute cosine similarities\n",
    "print(\"Computing cosine similarities...\")\n",
    "\n",
    "def compute_avg_similarity(pred_embeddings, true_embeddings):\n",
    "    similarities = util.cos_sim(true_embeddings, pred_embeddings).diagonal()\n",
    "    return similarities.mean().item()\n",
    "\n",
    "gpt_avg_sim = compute_avg_similarity(gpt_embeddings, ground_truth_embeddings)\n",
    "sarvam_avg_sim = compute_avg_similarity(sarvam_embeddings, ground_truth_embeddings)\n",
    "gemini_avg_sim = compute_avg_similarity(gemini_embeddings, ground_truth_embeddings)\n",
    "\n",
    "# Prepare summary table\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model': ['GPT', 'Sarvam', 'Gemini'],\n",
    "    'Average Similarity': [gpt_avg_sim, sarvam_avg_sim, gemini_avg_sim]\n",
    "})\n",
    "\n",
    "# Rank: higher similarity is better â†’ rank in descending order\n",
    "summary_df['Rank'] = summary_df['Average Similarity'].rank(method='min', ascending=False)\n",
    "\n",
    "# Save results\n",
    "summary_df.to_excel(\"Intent_Similarity_Comparison_LaBSE.xlsx\", index=False)\n",
    "summary_df.to_csv(\"Intent_Similarity_Comparison_LaBSE.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Done! Comparison saved to Intent_Similarity_Comparison_LaBSE.xlsx and .csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753c984f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding embeddings without normalization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  7.85it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  2.96it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  2.50it/s]\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:03<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample vector norms:\n",
      "Ground Truth norm: 6.227839946746826\n",
      "GPT norm: 6.227839946746826\n",
      "Sarvam norm: 6.227839946746826\n",
      "Gemini norm: 6.227839946746826\n",
      "\n",
      "âœ… Done! Dot products saved to DotProduct_ParaphraseModel.xlsx and .csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#code for dot product using paraphrase model which does not have internal normalisation\n",
    "#labse had internal normalisation thing in it\n",
    "\n",
    "# Required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Load a model that does NOT normalize by default\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "\n",
    "# Load the datasets\n",
    "ground_truth_df = pd.read_excel(\"unseen_data1xlsx.xlsx\")\n",
    "gpt_df = pd.read_excel(\"Gpt_Unseen.xlsx\")\n",
    "sarvam_df = pd.read_excel(\"Sarvam_Unseen.xlsx\")\n",
    "gemini_df = pd.read_excel(\"Gemini_Unseen.xlsx\")\n",
    "\n",
    "# Extract intent columns as string\n",
    "ground_truth_intents = ground_truth_df['002_pa'].astype(str).tolist()\n",
    "gpt_intents = gpt_df['Intent'].astype(str).tolist()\n",
    "sarvam_intents = sarvam_df['Intent'].astype(str).tolist()\n",
    "gemini_intents = gemini_df['Intent'].astype(str).tolist()\n",
    "\n",
    "# Encode embeddings WITHOUT normalization\n",
    "print(\"Encoding embeddings without normalization...\")\n",
    "ground_truth_embeddings = model.encode(ground_truth_intents, convert_to_tensor=True, normalize_embeddings=False, show_progress_bar=True)\n",
    "gpt_embeddings = model.encode(gpt_intents, convert_to_tensor=True, normalize_embeddings=False, show_progress_bar=True)\n",
    "sarvam_embeddings = model.encode(sarvam_intents, convert_to_tensor=True, normalize_embeddings=False, show_progress_bar=True)\n",
    "gemini_embeddings = model.encode(gemini_intents, convert_to_tensor=True, normalize_embeddings=False, show_progress_bar=True)\n",
    "\n",
    "# ðŸ” Sanity check: print vector norms\n",
    "print(\"\\nSample vector norms:\")\n",
    "print(\"Ground Truth norm:\", torch.norm(ground_truth_embeddings[0]).item())\n",
    "print(\"GPT norm:\", torch.norm(gpt_embeddings[0]).item())\n",
    "print(\"Sarvam norm:\", torch.norm(sarvam_embeddings[0]).item())\n",
    "print(\"Gemini norm:\", torch.norm(gemini_embeddings[0]).item())\n",
    "\n",
    "# ðŸ”¢ Compute dot product only\n",
    "def compute_avg_dot(pred_embeddings, true_embeddings):\n",
    "    dot_products = (true_embeddings * pred_embeddings).sum(dim=1)\n",
    "    return dot_products.mean().item()\n",
    "\n",
    "gpt_avg_dot = compute_avg_dot(gpt_embeddings, ground_truth_embeddings)\n",
    "sarvam_avg_dot = compute_avg_dot(sarvam_embeddings, ground_truth_embeddings)\n",
    "gemini_avg_dot = compute_avg_dot(gemini_embeddings, ground_truth_embeddings)\n",
    "\n",
    "# ðŸ“Š Save results\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model': ['GPT', 'Sarvam', 'Gemini'],\n",
    "    'Average Dot Product': [gpt_avg_dot, sarvam_avg_dot, gemini_avg_dot]\n",
    "})\n",
    "\n",
    "summary_df.to_excel(\"DotProduct_ParaphraseModel.xlsx\", index=False)\n",
    "summary_df.to_csv(\"DotProduct_ParaphraseModel.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ… Done! Dot products saved to DotProduct_ParaphraseModel.xlsx and .csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired t-test:   t = -13.0224,  p = 5.8452e-03\n",
      "Wilcoxon test:   W = 0.0000,  p = 2.5000e-01\n",
      "â‡’ The difference in average similarities is statistically significant (p < 0.05).\n"
     ]
    }
   ],
   "source": [
    "#Code for running the t-test for proving that LaBSE and Paraphrase model have significant results gap\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "\n",
    "# 1. Load your two CSVs:\n",
    "#    â€“ replace these filenames with the actual paths to your SBERT vs. LaBSE files\n",
    "labse_df = pd.read_csv(\"Intent_Similarity_Comparison_LaBSE.csv\")\n",
    "sbert_df = pd.read_csv(\"Comparison_Intents_Cosine.csv\")\n",
    "\n",
    "# 2. Extract the Average Similarity columns\n",
    "#    (make sure the column name matches exactly)\n",
    "labse_avgs = labse_df[\"Average Similarity\"]\n",
    "sbert_avgs = sbert_df[\"Average Similarity\"]\n",
    "\n",
    "# 3. Paired t-test\n",
    "t_stat, p_value = ttest_rel(labse_avgs, sbert_avgs)\n",
    "print(f\"Paired t-test:   t = {t_stat:.4f},  p = {p_value:.4e}\")\n",
    "\n",
    "# 4. (Optional) Non-parametric alternative: Wilcoxon signed-rank\n",
    "w_stat, w_p = wilcoxon(labse_avgs, sbert_avgs)\n",
    "print(f\"Wilcoxon test:   W = {w_stat:.4f},  p = {w_p:.4e}\")\n",
    "\n",
    "# 5. Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"â‡’ The difference in average similarities is statistically significant (p < 0.05).\")\n",
    "else:\n",
    "    print(\"â‡’ No statistically significant difference detected (p â‰¥ 0.05).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3c1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0efd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
